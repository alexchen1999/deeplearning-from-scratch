{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUHuche5aE3xTuCbm+XvCT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexchen1999/deeplearning-from-scratch/blob/main/transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Gf_KLBfSgA-2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  '''\n",
        "  Split an embedding into different parts\n",
        "\n",
        "  E.g. if we had an embedding size of 256 and 8 attention heads then the\n",
        "  input embedding would be split into 8 parts of size 32\n",
        "\n",
        "  '''\n",
        "  def __init__(self, embedding_size, n_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dimension = embedding_size // n_heads # integer division\n",
        "\n",
        "    # Q, K, V\n",
        "    self.values = nn.Linear(self.head_dimension, self.head_dimension, bias=False) # attention matrix --> maps head dimension to head dimension for split embedding\n",
        "    self.keys = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
        "    self.queries = nn.Linear(self.head_dimension, self.head_dimension, bias=False)\n",
        "\n",
        "    self.fc_out = nn.Linear(self.n_heads * self.head_dimension, self.embedding_size) # mutiplication between n_heads and head_dimension is some convention to make it clear we're concatenating the heads back to the original embedding size\n",
        "\n",
        "  def forward(self, keys, values, query, mask):\n",
        "    batch_size = query.shape[0] # How many examples we send at once\n",
        "    value_length, key_length, query_length = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "    # Objective is to reshape the Q,K,V tensors so that attention can be applied to multiple heads\n",
        "    # Original input size is (batch_size, length, embedding_size) for each I believe (see: https://pytorch.org/docs/stable/generated/torch.reshape.html)\n",
        "    # split embedding into n_heads --> does this mean that the embedding size must be cleanly divisible by the number of heads\n",
        "    values = values.reshape(batch_size, value_length, self.n_heads, self.head_dimension)\n",
        "    keys = keys.reshape(batch_size, key_length, self.n_heads, self.head_dimension)\n",
        "    query = query.reshape(batch_size, query_length, self.n_heads, self.head_dimension)\n",
        "\n",
        "    # Objective: Multiply queries with the keys in attention formula, but need to prepare tensor dimensions for batch matrix multiplication.\n",
        "    # One way to do it easily is with einsum. The einsum operation \"nqhd,nkhd->nhqk\" essentially computes the dot product between every query and key pair across all batches and heads.\n",
        "    # QK = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "    # (batch_size, query_length, n_heads, head_dimension)\n",
        "    # (batch_size, key_length, n_heads, head_dimension)\n",
        "    # Output of this matmul is (batch_size, n_heads, query_length, key_length)\n",
        "\n",
        "    # Equivalently one could manually emulate einsum operation:\n",
        "    # Step 1: Transpose the heads and sequence length dimensions to align them for batch matrix multiplication\n",
        "    query = query.transpose(1, 2)  # New shape: [batch_size, n_heads, seq_len, head_dim]\n",
        "    keys = keys.transpose(1, 2)  # Also, [batch_size, n_heads, seq_len, head_dim]\n",
        "\n",
        "    # Before performing bmm, keys need to be transposed so that the last two dimensions are [head_dim, seq_len]\n",
        "    # We're interested in computing the attention between the sequence lengths of queries and keys which is why we need to transpose with head_dim to satisfy inner product dimensions\n",
        "    keys = keys.transpose(2, 3)  # New shape: [batch_size, n_heads, head_dim, seq_len]\n",
        "\n",
        "    # Next have to combine batch size and heads into one dimension because torch.bmm expects a 3D input\n",
        "    query_flattened = query.reshape(batch_size * self.n_heads, query_length, self.head_dimension)\n",
        "    keys_flattened = keys.reshape(batch_size * self.n_heads, self.head_dimension, key_length)\n",
        "\n",
        "    # Step 2: Perform batch matrix multiplication\n",
        "    # Output shape after bmm: [batch_size, n_heads, seq_len (from queries), seq_len (from keys)]\n",
        "    # Energy/QK intepretation: for each word in our target (query) sentence, how much should we pay attention to each word in our source sentence\n",
        "    QK = torch.bmm(query_flattened, keys_flattened)\n",
        "\n",
        "    # Unflatten to get back batch size and n_heads\n",
        "    QK = QK.reshape(batch_size, self.n_heads, query_length, key_length)\n",
        "\n",
        "    # Mask certain elements with \"-inf\" (in practice a very small number)\n",
        "    # https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_\n",
        "    if mask is not None:\n",
        "      QK = QK.masked_fill_(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "    # Compute attention across the key length\n",
        "    attention = torch.softmax(QK / (self.embedding_size ** (1/2)), dim=3)\n",
        "\n",
        "    # attention = [batch_size, n_heads, query_length, key_length]\n",
        "    # values = [batch_size, value_length, n_heads, head_dimension]\n",
        "    # output = [batch_size, query_length, n_heads, head_dimension]\n",
        "\n",
        "    attention_flattened = attention.reshape(batch_size * self.n_heads, query_length, key_length)\n",
        "    values_flattened = values.reshape(batch_size * self.n_heads, value_length, self.head_dimension)\n",
        "\n",
        "    # batch matmul\n",
        "    output_flattened = torch.bmm(attention_flattened, values_flattened)\n",
        "    output = output_flattened.reshape(batch_size, query_length, self.n_heads * self.head_dimension)\n",
        "\n",
        "    output = self.fc_out(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "idQn1L9_gKDw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing a la torch.bmm example: https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm\n",
        "\n",
        "batch_size = 3\n",
        "n_heads = 2\n",
        "head_dimension = 10\n",
        "query_length = 256 # Some example 256 word query sentence\n",
        "key_length = 32\n",
        "value_length = 32 # Key length and value length should be the same since they're pairs\n",
        "\n",
        "'''\n",
        "torch.bmm expects both tensors to be 3D with shapes (batch_size, *, *), where the first dimension is the batch size,\n",
        "and the next two dimensions are the matrix dimensions that will be multiplied together. In a multi-head attention scenario,\n",
        "you have an additional \"heads\" dimension, so you'll need to reshape your tensors to temporarily combine the batch and heads\n",
        "dimensions into one, perform the batch matrix multiplication, then reshape the result back to separate the batch and heads dimensions.\n",
        "\n",
        "'''\n",
        "\n",
        "attention = torch.randn(batch_size, n_heads, query_length, key_length)\n",
        "values = torch.randn(batch_size, value_length, n_heads, head_dimension)\n",
        "\n",
        "attention_flattened = attention.reshape(batch_size * n_heads, query_length, key_length)\n",
        "values_flattened = values.reshape(batch_size * n_heads, value_length, head_dimension)\n",
        "\n",
        "print(attention.shape)\n",
        "print(values.shape)\n",
        "\n",
        "print(attention_flattened.shape)\n",
        "print(values_flattened.shape)\n",
        "\n",
        "output_flattened = torch.bmm(attention_flattened, values_flattened)\n",
        "print(output_flattened.shape)\n",
        "\n",
        "output = output_flattened.reshape(batch_size, query_length, n_heads, head_dimension)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUEh1nCPQ8Mu",
        "outputId": "790ef1cf-3eea-4dea-b496-878e240ef613"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 256, 32])\n",
            "torch.Size([3, 32, 2, 10])\n",
            "torch.Size([6, 256, 32])\n",
            "torch.Size([6, 32, 10])\n",
            "torch.Size([6, 256, 10])\n",
            "torch.Size([3, 256, 2, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(attention_flattened)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2XzJOIxRI0W",
        "outputId": "d67a4e75-f632-4d22-a4b7-3e7e79446e64"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.8661, -0.3334,  2.1608,  ...,  0.0204, -0.8349,  0.4355],\n",
            "         [-0.1408, -0.2549, -0.9407,  ...,  1.2538,  0.5239, -0.5087],\n",
            "         [ 0.9114, -0.0209, -1.1141,  ...,  2.1751, -0.2217,  0.3731],\n",
            "         ...,\n",
            "         [ 0.7588, -0.1412, -1.0503,  ...,  1.2238, -0.7840, -0.3918],\n",
            "         [-0.0598, -0.6259, -1.5142,  ...,  0.7198, -1.4527,  0.0581],\n",
            "         [ 0.6848,  1.0335, -0.5327,  ..., -1.0223, -0.6783,  0.3035]],\n",
            "\n",
            "        [[ 1.5701,  0.0271, -0.0106,  ..., -0.1874,  0.2687, -0.7438],\n",
            "         [-0.0702, -0.6042,  0.0681,  ..., -0.3650, -0.1657,  0.3115],\n",
            "         [-0.2184,  1.3624,  0.3536,  ..., -0.3137,  0.2350, -1.0870],\n",
            "         ...,\n",
            "         [ 0.5249, -0.6473,  0.5684,  ..., -0.1210,  0.7735,  0.1469],\n",
            "         [ 0.9371, -1.0981, -0.8474,  ...,  0.7108, -2.0175, -1.4260],\n",
            "         [ 0.4808, -1.7110, -0.2402,  ...,  0.0283,  0.5715,  0.0600]],\n",
            "\n",
            "        [[-0.2092, -0.9602, -0.0471,  ..., -0.8422, -0.3692,  0.1834],\n",
            "         [-1.1549, -1.3857,  1.4099,  ...,  2.0215,  0.0881, -1.6863],\n",
            "         [ 1.0364,  0.7049,  0.8956,  ...,  0.0471, -0.2671,  0.2717],\n",
            "         ...,\n",
            "         [ 0.6727,  0.3920,  0.2922,  ...,  1.0105,  0.2767, -0.3511],\n",
            "         [-1.5282, -0.1121, -1.6492,  ...,  0.2976, -0.1309, -0.3096],\n",
            "         [ 0.3554,  0.0717, -0.0853,  ..., -0.5766,  0.9023,  0.2924]],\n",
            "\n",
            "        [[ 0.8030,  1.0984, -1.4030,  ..., -0.4645,  0.9170,  0.3992],\n",
            "         [ 0.4479,  0.6043,  0.4727,  ...,  1.6259,  1.2542, -1.7616],\n",
            "         [ 0.0257, -1.7074,  0.9273,  ...,  1.2486, -1.0106, -0.8711],\n",
            "         ...,\n",
            "         [-0.3786,  0.5899, -0.5422,  ..., -0.1420,  0.3747,  0.6211],\n",
            "         [-2.2443,  0.0681,  1.4790,  ..., -0.5848,  0.1439, -0.7673],\n",
            "         [-0.5329,  0.0495, -1.3042,  ..., -0.9712, -0.6398,  0.3327]],\n",
            "\n",
            "        [[ 2.0167, -0.3111, -0.1996,  ...,  0.7716,  3.5609, -1.2200],\n",
            "         [-0.3618,  1.5569,  1.6138,  ...,  1.5786,  1.2604, -1.4012],\n",
            "         [-0.9366,  1.2745, -0.8794,  ..., -0.5418,  0.3071, -0.7395],\n",
            "         ...,\n",
            "         [ 0.5981, -0.4208, -0.6070,  ...,  1.1323, -0.8987,  0.6396],\n",
            "         [ 0.8162,  1.0222,  0.5876,  ...,  0.6842, -0.1428, -0.9043],\n",
            "         [-0.4215, -0.2963,  0.8401,  ..., -3.2244, -1.4921, -1.3434]],\n",
            "\n",
            "        [[ 0.8387, -0.7796, -0.7479,  ..., -1.3115,  0.1396, -0.4382],\n",
            "         [ 0.5581,  0.7820, -0.2468,  ...,  1.5870,  0.1235,  0.0606],\n",
            "         [-0.8461, -1.5200, -0.0364,  ...,  2.2874,  1.0150,  1.0357],\n",
            "         ...,\n",
            "         [ 0.5605,  1.2259,  1.1345,  ...,  1.0990, -0.7414, -0.3442],\n",
            "         [ 0.4407,  1.3435,  1.0094,  ..., -0.6995,  0.7218, -0.7416],\n",
            "         [-0.2697, -0.8306,  1.2329,  ..., -2.1958,  0.4200,  1.8528]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTUIPNLMXwuO",
        "outputId": "8d334049-a1f4-4434-dea9-a0b8d40a3863"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ -0.1981,  19.6662,   2.3831,  ...,  -0.6509, -18.2371,  -9.5532],\n",
            "          [ -8.7651,  -6.2180,  -2.6235,  ..., -10.6252,   6.7315,   8.9361]],\n",
            "\n",
            "         [[ -0.6194,  -2.7457,   1.3941,  ...,  -1.6666,   7.6069,   5.1755],\n",
            "          [ 12.5339,  -4.0733,   0.7828,  ...,  -7.7826,   4.9962, -11.8000]],\n",
            "\n",
            "         [[  4.8412,  -2.6500,   3.8612,  ...,   5.8582,  -0.1182,   0.5946],\n",
            "          [  1.2622,   5.7494,  -0.3883,  ...,   7.0686, -10.2198,  -2.7761]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ -0.2550,  -7.1119,  -3.6585,  ...,  -3.2007,   0.4311,  -5.5778],\n",
            "          [ -2.7743,  -2.8833,  -7.1750,  ...,  -5.6871,  -5.8650,  -0.4186]],\n",
            "\n",
            "         [[ -6.9740,  -4.3421,  -1.2973,  ...,   1.4527,  -2.8657,  -8.5665],\n",
            "          [ -2.1417,   4.9784,   2.3405,  ...,  -4.5241,  -7.0783,   9.0825]],\n",
            "\n",
            "         [[ -1.6823,   2.0054,   7.2486,  ...,   3.0606,   5.0798,   1.0843],\n",
            "          [  0.9063,   5.0518,   0.1211,  ..., -10.1762, -10.4950,  -0.0917]]],\n",
            "\n",
            "\n",
            "        [[[ -5.7294,   2.8302,  -9.2000,  ...,  -3.6078,  -0.9531,   1.7076],\n",
            "          [ -6.5540,  -9.2464,   1.3205,  ...,  -0.8982,  -1.1046,  -2.0449]],\n",
            "\n",
            "         [[  1.5734,  -3.3689,  -6.2233,  ...,   4.9360,  -5.3415,   0.7760],\n",
            "          [ 12.6599,   0.5985,  -7.9991,  ...,  11.3764,  -3.5807,   2.0297]],\n",
            "\n",
            "         [[ -3.7685,  -2.3201,   0.0311,  ...,   2.6889,  -6.4694,  -6.0415],\n",
            "          [ -3.5327,   1.3286,  -6.1098,  ...,   4.6029,  -9.1849,  -0.9843]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[  5.7632,   4.5151,  -4.2456,  ...,  -0.5921,   2.2278,   5.7155],\n",
            "          [  2.9330,  -1.8623,   7.8527,  ...,  12.0440,  -3.8104,  -3.9479]],\n",
            "\n",
            "         [[ -8.3565,   5.2001,   5.2617,  ...,  -0.0944,   2.7027,  -2.0834],\n",
            "          [-12.8193,  10.5005,  -6.1301,  ...,   5.6442,   2.6011,  -3.1960]],\n",
            "\n",
            "         [[  0.1826,  -8.2292,   2.9883,  ...,  -7.3466,   2.7034,   2.3559],\n",
            "          [  6.1639,   6.8141,  -7.0631,  ...,  -8.2442,   4.1142,  -1.2268]]],\n",
            "\n",
            "\n",
            "        [[[ 11.7083,  -3.3230,  -4.5835,  ...,  12.3674, -17.3784,   1.9235],\n",
            "          [  3.7238,  -0.9339,  -2.5178,  ...,   8.7840,  -5.9905,  -9.3978]],\n",
            "\n",
            "         [[  4.2925,   5.1886,  -1.1684,  ...,   0.7893,   3.8685,   7.3889],\n",
            "          [ -1.7782,  -0.5739,   1.6465,  ...,   5.5113,  -4.4305,   5.6290]],\n",
            "\n",
            "         [[ -0.2442,   4.8234,   4.1212,  ...,   4.3468,  -1.1398,  -3.7352],\n",
            "          [ -1.4725,   3.3250,   2.7888,  ...,  -5.0010,   2.1603,   5.7764]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[  5.5210, -11.1497,  -3.0141,  ...,   5.8292,   3.4485,   2.9669],\n",
            "          [ -3.9272, -10.9775,  -3.2481,  ...,   2.7792,   2.9408,  -7.2975]],\n",
            "\n",
            "         [[  7.5502, -10.1518,   0.2636,  ...,  -0.9592,   4.5914,   0.5734],\n",
            "          [  9.9884,   7.8345,   0.0437,  ...,  -4.2698,  -4.5292,   2.7062]],\n",
            "\n",
            "         [[ -1.9974,   4.7733,  -5.6937,  ..., -10.5762, -11.6029,  -3.2825],\n",
            "          [  7.1495,   6.3759,  -7.5825,  ...,  -0.6995,   3.5200,  -3.1316]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embedding_size, n_heads, dropout, forward_expansion):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "\n",
        "    print(embedding_size, n_heads)\n",
        "\n",
        "    self.mha = MultiHeadAttention(embedding_size, n_heads)\n",
        "    self.norm1 = nn.LayerNorm(embedding_size) # https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html, reference: https://arxiv.org/abs/1607.06450\n",
        "                                              # SImilar to torch.nn.BatchNorm2D -- only difference is BatchNorm normalizes within a batch of samples whereas LayerNorm is just within a sample.\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(embedding_size, forward_expansion * embedding_size), # map the embedding size to the forward expansion times the embedding size; in original paper value of forward expansion is 4x.\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(forward_expansion * embedding_size, embedding_size)  # map it back to original embedding size.\n",
        "    )\n",
        "\n",
        "    self.norm2 = nn.LayerNorm(embedding_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, keys, values, query, mask):\n",
        "    mha = self.mha(values, keys, query, mask) # How does this line make sense\n",
        "    norm1 = self.dropout(self.norm1(mha + query)) # Add a residual connection and dropout\n",
        "    feed_forward = self.feed_forward(norm1)\n",
        "    output = self.dropout(self.norm2(feed_forward + norm1)) # Add another residual connection\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "w8BQJlM5YHIc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  '''\n",
        "  max_sentence_length is related to the positional embedding. Send in how long is the max sentence length\n",
        "\n",
        "\n",
        "  '''\n",
        "  def __init__(self, src_vocab_size, max_sentence_length, embedding_size, n_layers, n_heads, device, forward_expansion, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.word_embedding = nn.Embedding(src_vocab_size, embedding_size) # [src_vocab_size, embedding_size]\n",
        "    self.positional_embedding = nn.Embedding(max_sentence_length, embedding_size) # [max_sentence_length, embedding_size]\n",
        "\n",
        "    print('encoder')\n",
        "    print(embedding_size, n_heads, dropout, forward_expansion)\n",
        "\n",
        "    self.layers = nn.ModuleList([\n",
        "        TransformerBlock(embedding_size, n_heads, dropout=dropout, forward_expansion=forward_expansion)\n",
        "    ])\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    N, seq_length = x.shape\n",
        "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device) # Positional embeddings: N x [0, 1, 2, ..., seq_length]\n",
        "\n",
        "    output = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      output = layer(output, output, output, mask)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, embedding_size, n_heads, forward_expansion, dropout, device):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(embedding_size, n_heads)\n",
        "    self.norm = nn.LayerNorm(embedding_size)\n",
        "    self.transformer_block = TransformerBlock(embedding_size, n_heads, dropout, forward_expansion)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x, value, key, src_mask, tgt_mask):\n",
        "    attention = self.mha(x, x, x, tgt_mask)\n",
        "    query = self.dropout(self.norm(x + attention))\n",
        "    output = self.transformer_block(value, key, query, src_mask)\n",
        "    return output\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, tgt_vocab_size, embedding_size, n_layers, n_heads, forward_expansion, dropout, device, max_sentence_length):\n",
        "      super(Decoder, self).__init__()\n",
        "\n",
        "      self.device = device\n",
        "      self.word_embedding = nn.Embedding(tgt_vocab_size, embedding_size)\n",
        "      self.positional_embedding = nn.Embedding(max_sentence_length, embedding_size)\n",
        "\n",
        "      self.layers = nn.ModuleList([DecoderBlock(embedding_size, n_heads, forward_expansion, dropout, device) for _ in range(n_layers)]) # N decoder blocks\n",
        "      self.fc_out = nn.Linear(embedding_size, tgt_vocab_size)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, encoder_out, src_mask, tgt_mask):\n",
        "    N, seq_length = x.shape\n",
        "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "    x = self.dropout((self.word_embedding(x) + self.positional_embedding(positions)))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_out, encoder_out, src_mask, tgt_mask)\n",
        "\n",
        "    output = self.fc_out(x)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "7I9DqogHh9uw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, src_pad_idx, tgt_pad_idx, embedding_size=256, n_layers=6, forward_expansion=4, n_heads=8, dropout=0, device='cuda', max_sentence_length=100):\n",
        "\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(src_vocab_size=src_vocab_size, embedding_size=embedding_size, n_layers=n_layers, n_heads=n_heads, device=device, forward_expansion=forward_expansion, dropout=dropout, max_sentence_length=max_sentence_length)\n",
        "    self.decoder = Decoder(tgt_vocab_size=tgt_vocab_size, embedding_size=embedding_size, n_layers=n_layers, n_heads=n_heads, forward_expansion=forward_expansion, dropout=dropout, device=device, max_sentence_length=max_sentence_length)\n",
        "\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.tgt_pad_idx = tgt_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    return src_mask.to(self.device)\n",
        "\n",
        "  def make_tgt_mask(self, tgt):\n",
        "    N, tgt_length = tgt.shape\n",
        "    tgt_mask = torch.tril(torch.ones((tgt_length, tgt_length))).expand(N, 1, tgt_length, tgt_length)\n",
        "    return tgt_mask.to(self.device)\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    tgt_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "    src_encoder = self.encoder(src, src_mask)\n",
        "    output = self.decoder(tgt, src_encoder, src_mask, tgt_mask)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "GaOn3GPbxsDe"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1], [2], [3]])\n",
        "print(x.size())\n",
        "x.expand(3, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWGszPQRy1Xq",
        "outputId": "7a4e7a8a-8115-4df0-8f22-457a8fdd232f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1],\n",
              "        [2, 2, 2, 2],\n",
              "        [3, 3, 3, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1], [2], [3]])\n",
        "print(x)\n",
        "x.expand(3, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-zIKVMzy5AD",
        "outputId": "bc38e2ec-44ad-4c4d-f110-ddb23b544bbb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1],\n",
            "        [2],\n",
            "        [3]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1],\n",
              "        [2, 2, 2, 2, 2],\n",
              "        [3, 3, 3, 3, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
        "    device\n",
        ")\n",
        "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
        "\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "src_vocab_size = 10\n",
        "trg_vocab_size = 10\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
        "    device\n",
        ")\n",
        "out = model(x, trg[:, :-1])\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aeWiEl6zPoI",
        "outputId": "e6455c39-ce71-4ea2-fa2e-40de5ec2ffb2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "encoder\n",
            "256 8 0 4\n",
            "256 8\n",
            "256 8\n",
            "256 8\n",
            "256 8\n",
            "256 8\n",
            "256 8\n",
            "256 8\n",
            "torch.Size([2, 7, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ffJsxAp72uz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}